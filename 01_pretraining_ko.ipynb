{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<a href='https://honglab.ai'><p style=\"text-align:center;\"><img src='https://lh3.googleusercontent.com/lY3ySXooSmwsq5r-mRi7uiypbo0Vez6pmNoQxMFhl9fmZJkRHu5lO2vo7se_0YOzgmDyJif9fi4_z0o3ZFdwd8NVSWG6Ea80uWaf3pOHpR4GHGDV7kaFeuHR3yAjIJjDgfXMxsvw=w2400'  class=\"center\" width=\"50%\" height=\"50%\"/></p></a>\n",
    "___\n",
    "<center><em>Content Copyright by HongLab, Inc.</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÎåÄÌòïÏñ∏Ïñ¥Î™®Îç∏(LLM) Î∞îÎã•Î∂ÄÌÑ∞ ÎßåÎì§Í∏∞\n",
    "\n",
    "[Ïú†ÌäúÎ∏å Í∞ïÏùò ÏòÅÏÉÅ ÎßÅÌÅ¨](https://youtu.be/osv2csoHVAo)\n",
    "\n",
    "[ÌôçÏ†ïÎ™® Ïó∞Íµ¨ÏÜå ÎîîÏä§ÏΩîÎìú ÎßÅÌÅ¨](https://discord.com/invite/kgR9xJkbsV)\n",
    "\n",
    "[ÌôçÏ†ïÎ™® Ïó∞Íµ¨ÏÜå ÌôàÌéòÏù¥ÏßÄ ÎßÅÌÅ¨](https://www.honglab.ai/)\n",
    "\n",
    "#### Ï∞∏Í≥† ÏûêÎ£å\n",
    "- [Andrej Karpathy Ïú†ÌäúÎ∏å](https://www.youtube.com/andrejkarpathy)\n",
    "- [Build a Large Language Model (From Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch)\n",
    "- [Om-Alve/smolGPT ÍπÉÌóô](https://github.com/Om-Alve/smolGPT)\n",
    "- Ìä∏ÎûúÏä§Ìè¨Î®∏ ÎÖºÎ¨∏ - [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "- OpenAI GPT2 ÎÖºÎ¨∏ - [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ÏïàÎÇ¥ÏÇ¨Ìï≠\n",
    "\n",
    "LLMÏùò ÌïµÏã¨ Í∞úÎÖêÏùÑ Í∞úÏù∏ PCÏóêÏÑúÎèÑ Í∞ÑÎã®ÌïòÍ≤å Ïã§ÏäµÌïòÎ©¥ÏÑú Í≥µÎ∂ÄÌï† Ïàò ÏûàÎäî ÌïôÏäµ ÏûêÎ£åÏûÖÎãàÎã§. ÎÑêÎ¶¨ ÏïåÎ†§ÏßÑ ÍµêÏú°/ÌïôÏà† ÏûêÎ£åÎì§ÏùÑ Ï∞∏Í≥†ÌïòÏó¨ ÏâΩÍ≤å Í≥µÎ∂ÄÌï† Ïàò ÏûàÎèÑÎ°ù ÏöîÏïΩÌïòÍ≥† Ï†ïÎ¶¨Ìïú Í≤ÉÏûÖÎãàÎã§. ÏΩîÎî© Ïä§ÌÉÄÏùºÏù¥ÎÇò ÌôúÏö© Î≤îÏúÑÏóê ÎåÄÌï¥ Ïò§Ìï¥ ÏóÜÏúºÏãúÍ∏∏ Î∞îÎûçÎãàÎã§.\n",
    "\n",
    "ÏúàÎèÑÏö∞11/WSL, Python 3.9.20, Pytorch 2.6, CUDA 12.6 ÏóêÏÑú ÏûëÎèôÏùÑ ÌôïÏù∏ÌïòÏòÄÏäµÎãàÎã§. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ï†ÑÏ≤¥ Í≥ºÏ†ï ÏöîÏïΩ\n",
    "\n",
    "LLM Í∏∞Î∞ò AI ÏóêÏù¥Ï†ÑÌä∏Î•º ÎßåÎì§ÎïåÎäî ÌïµÏã¨Ïù¥ ÎêòÎäî LLMÏù¥ ÌïÑÏöîÌïúÎç∞Ïöî, LLMÏùÑ Î∞îÎã•Î∂ÄÌÑ∞ ÎßåÎìúÎäî Í≤ΩÏö∞ Î≥¥Îã§Îäî Í≥µÍ∞úÎêòÏñ¥ ÏûàÎäî LLM Î™®Îç∏Îì§ÏùÑ Í∞ÄÏ†∏Îã§Í∞Ä ÎÇòÏùò Ïö©ÎèÑÏóê ÎßûÎèÑÎ°ù Îã§Îì¨Ïñ¥ÏÑú ÏÇ¨Ïö©ÌïòÎäî Í≤ÉÏù¥ ÏùºÎ∞òÏ†ÅÏûÖÎãàÎã§. Îã§Îßå, ÏµúÍ∑ºÏóêÎäî LLMÏùÑ Î∞îÎã•Î∂ÄÌÑ∞ ÎßåÎìúÎäî Í∏∞Ïà†Ïóê ÎåÄÌïú ÏßÑÏûÖÏû•Î≤ΩÏù¥ ÎÇÆÏïÑÏßÄÍ≥† ÏûàÏñ¥ÏÑú ÌöåÏÇ¨Î≥ÑÎ°ú ÌïÑÏöîÌïú LLMÏùÑ Î∞îÎã•Î∂ÄÌÑ∞ Í∞ÅÏûê ÎßåÎì§Ïñ¥ ÏÇ¨Ïö©ÌïòÍ≤å Îê† Í∞ÄÎä•ÏÑ±ÎèÑ ÎÜíÏïÑÏßÄÍ≥† ÏûàÏäµÎãàÎã§.\n",
    "\n",
    "LLMÏùÑ ÎßåÎì§ ÎïåÎäî \n",
    "\n",
    "1. ÏÇ¨Ï†ÑÌõàÎ†®(pretraining)ÏúºÎ°ú ÏùºÎ∞òÏ†ÅÏù∏ Ïñ∏Ïñ¥ Îä•Î†•ÏùÑ Í∞ÄÎ•¥Ïπú ÌõÑÏóê \n",
    "2. ÎØ∏ÏÑ∏Ï°∞Ï†ï(fine tuning) Îã®Í≥ÑÏóêÏÑú ÌäπÏ†ï ÏóÖÎ¨¥Ïóê Ï†ÅÏùë\n",
    "\n",
    "ÏãúÌÇ§Îäî Í≤ÉÏù¥ Í∏∞Î≥∏Ïù¥ Îê©ÎãàÎã§. Ïó¨Í∏∞Ïóê \n",
    "\n",
    "3. Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§(+Ïù∏ÌÑ∞ÎÑ∑) Í≤ÄÏÉâ Í∏∞Îä•ÏùÑ Ï∂îÍ∞Ä\n",
    "\n",
    "ÌïòÎ©¥ ÏßÄÏãùÏùò Î≤îÏúÑÏôÄ Ï†ïÌôïÏÑ±ÏùÑ ÎÜíÏùº Ïàò ÏûàÏäµÎãàÎã§. ÏÇ¨ÎûåÏù¥ ÏÉùÍ∞ÅÏùÑ Í±∞Îì≠ÌïòÏó¨ Îçî ÍπäÏù¥ÏûàÎäî Í≤∞Î°†ÏùÑ Ïù¥ÎÅåÏñ¥ ÎÇ¥ÎìØÏù¥ LLMÎèÑ \n",
    "\n",
    "4. ÎÇ¥Î∂ÄÏ†ÅÏúºÎ°ú ÏßàÏùòÎ•º Î∞òÎ≥µÌïòÏó¨ Îçî Ï¢ãÏùÄ Í≤∞Î°†ÏùÑ ÎèÑÏ∂ú\n",
    "\n",
    "ÌïòÎèÑÎ°ù ÎßåÎì§ Ïàò ÏûàÏäµÎãàÎã§.\n",
    "\n",
    "Ïó¨Í∏∞ÏÑúÎäî LLMÏùò Í∏∞Î≥∏ ÏõêÎ¶¨Î•º Ïù¥Ìï¥ÌïòÍ∏∞ ÏúÑÌï¥ÏÑú ÏÇ¨Ï†ÑÌõàÎ†® Í≥ºÏ†ïÏùÑ Î∞îÎã•Î∂ÄÌÑ∞ ÏßÑÌñâÌï¥Î≥¥Í≤†ÏäµÎãàÎã§. ÌõàÎ†® Í≥ºÏ†ïÏùò ÌÅ∞ ÌãÄÏùÄ ÏùºÎ∞òÏ†ÅÏù∏ Î®∏Ïã†Îü¨Îãù Ï†àÏ∞®Î•º Îî∞Î¶ÖÎãàÎã§.\n",
    "\n",
    "1. ÌõàÎ†® Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ\n",
    "1. Îç∞Ïù¥ÌÑ∞ Î°úÎçî Ï†ïÏùò\n",
    "1. Î™®Îç∏ Ï†ïÏùò\n",
    "1. ÌõàÎ†®\n",
    "1. Í≤∞Í≥º ÌôïÏù∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ÌõàÎ†® Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ\n",
    "\n",
    "Ï§ÄÎπÑÌïú ÌÖçÏä§Ìä∏ ÌååÏùºÏùÑ ÏùΩÏñ¥ Îì§Ïó¨ÏÑú Ï†ïÎ¶¨Ìïú ÌõÑÏóê ÏïûÏóê cleaned_Í∞Ä Î∂ôÏùÄ ÌååÏùº Ïù¥Î¶ÑÏúºÎ°ú Ï†ïÎ¶¨Ìï©ÎãàÎã§.\n",
    "> ÏòàÏãú) alice.txt &rarr; cleaned_alice.txt\n",
    "\n",
    "- Ï∫êÍ∏Ä Ìï¥Î¶¨Ìè¨ÌÑ∞ Ï±Ö - [Harry Potter Books](https://www.kaggle.com/datasets/shubhammaindola/harry-potter-books?select=02+Harry+Potter+and+the+Chamber+of+Secrets.txt)\n",
    "- Ï∫êÍ∏Ä Ïï®Î¶¨Ïä§ Ï±Ö - [alice.txt](https://www.kaggle.com/datasets/leelatte/alicetxt)\n",
    "- ÌõàÎ†® Îç∞Ïù¥ÌÑ∞ÎÇò Í∞ÄÏ§ëÏπòÎäî Ï†úÍ∞Ä Î∞∞Ìè¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§. ÏßÅÏ†ë Îã§Ïö¥Î∞õÍ±∞ÎÇò Ï§ÄÎπÑÌïòÏÖîÏïºÌï©ÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_ko_novel.txt 1138003 characters\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        book_text = file.read()\n",
    "\n",
    "    cleaned_text = re.sub(r'\\n+', ' ', book_text) # Ï§ÑÎ∞îÍøàÏùÑ ÎπàÏπ∏ÏúºÎ°ú Î≥ÄÍ≤Ω\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text) # Ïó¨Îü¨ ÎπàÏπ∏ÏùÑ ÌïòÎÇòÏùò ÎπàÏπ∏ÏúºÎ°ú\n",
    "\n",
    "    print(\"cleaned_\" + filename, len(cleaned_text), \"characters\") # Í∏ÄÏûê Ïàò Ï∂úÎ†•\n",
    "\n",
    "    with open(\"cleaned_\" + filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(cleaned_text)\n",
    "\n",
    "filenames_list = [\"ko_novel.txt\"]\n",
    "\n",
    "for filename in filenames_list:\n",
    "    clean_text(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ÌÜ†ÌÅ∞Ìôî\n",
    "\n",
    "UTF-8 BPE(Bype Pair Encoding)\n",
    "- GPT-2\n",
    "  - ÏÑúÎ∏åÏõåÎìú ÌÜ†ÌÅ¨ÎÇòÏù¥Ïßï: ÏûêÏ£º Îì±Ïû•ÌïòÏßÄ ÏïäÎäî Îã®Ïñ¥Îäî Îçî ÏûëÏùÄ Îã®ÏúÑÎ°ú Ï™ºÍ∞úÏßê\n",
    "  - Ìö®Ïú®ÏÑ±: ÏûêÏ£º Îì±Ïû•ÌïòÎäî Îã®Ïñ¥ÎÇò Íµ¨Îäî ÌïòÎÇòÏùò ÌÜ†ÌÅ∞ÏúºÎ°ú ÌëúÌòÑÎê®\n",
    "  - Îã§Íµ≠Ïñ¥ Ï≤òÎ¶¨: ÏòÅÏñ¥Í∞Ä ÏïÑÎãå Ïñ∏Ïñ¥Îäî ÌÜ†ÌÅ∞ÌôîÍ∞Ä Îçú Ìö®Ïú®Ï†ÅÏùº Ïàò ÏûàÏùå\n",
    "\n",
    "- ÌÜ†ÌÅ∞Ìôî Îêú Ïà´ÏûêÎì§ÏùÄ GPT-2 ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÏùò Ïñ¥Ìúò ÏÇ¨Ï†Ñ(vocabulary)ÏóêÏÑú Í∞Å ÌÜ†ÌÅ∞Ïóê Ìï†ÎãπÎêú Í≥†Ïú† IDÏûÑ.\n",
    "- ÏòàÎ•º Îì§Ïñ¥, \"Harry\"Îäî 18308Î≤à, \"Potter\"Îäî 14179Î≤à Í∞ôÏùÄ ÏãùÏúºÎ°ú Îß§ÌïëÎêòÏñ¥ ÏûàÏùå."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Í∏ÄÏûêÏàò: 220 ÌÜ†ÌÅ∞Ïàò 52\n",
      "[13059, 353, 318, 262, 33630, 1295, 284, 7808, 340, 13, 447, 251, 5850, 2067, 284, 1560, 606, 546, 18373, 11, 475, 19959, 19072, 13, 564, 250, 1135, 1541, 760, 851, 356, 2982, 8129, 11130, 261, 44906, 5149, 8129, 1610, 270, 16239, 428, 3329, 13, 1320, 447, 247, 82, 1521, 356, 3066, 356]\n",
      "potter is the safest place to hide it.‚Äù Harry started to tell them about Colin, but Hermione interrupted. ‚ÄúWe already know ‚Äî we heard Professor McGonagall telling Professor Flitwick this morning. That‚Äôs why we decided we\n",
      "13059\t -> pot\n",
      "353\t -> ter\n",
      "318\t ->  is\n",
      "262\t ->  the\n",
      "33630\t ->  safest\n",
      "1295\t ->  place\n",
      "284\t ->  to\n",
      "7808\t ->  hide\n",
      "340\t ->  it\n",
      "13\t -> .\n",
      "447\t -> ÔøΩ\n",
      "251\t -> ÔøΩ\n",
      "5850\t ->  Harry\n",
      "2067\t ->  started\n",
      "284\t ->  to\n",
      "1560\t ->  tell\n",
      "606\t ->  them\n",
      "546\t ->  about\n",
      "18373\t ->  Colin\n",
      "11\t -> ,\n",
      "475\t ->  but\n",
      "19959\t ->  Hermione\n",
      "19072\t ->  interrupted\n",
      "13\t -> .\n",
      "564\t ->  ÔøΩ\n",
      "250\t -> ÔøΩ\n",
      "1135\t -> We\n",
      "1541\t ->  already\n",
      "760\t ->  know\n",
      "851\t ->  ‚Äî\n",
      "356\t ->  we\n",
      "2982\t ->  heard\n",
      "8129\t ->  Professor\n",
      "11130\t ->  McG\n",
      "261\t -> on\n",
      "44906\t -> agall\n",
      "5149\t ->  telling\n",
      "8129\t ->  Professor\n",
      "1610\t ->  Fl\n",
      "270\t -> it\n",
      "16239\t -> wick\n",
      "428\t ->  this\n",
      "3329\t ->  morning\n",
      "13\t -> .\n",
      "1320\t ->  That\n",
      "447\t -> ÔøΩ\n",
      "247\t -> ÔøΩ\n",
      "82\t -> s\n",
      "1521\t ->  why\n",
      "356\t ->  we\n",
      "3066\t ->  decided\n",
      "356\t ->  we\n"
     ]
    }
   ],
   "source": [
    "import tiktoken # pip install tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = \"potter is the safest place to hide it.‚Äù Harry started to tell them about Colin, but Hermione interrupted. ‚ÄúWe already know ‚Äî we heard Professor McGonagall telling Professor Flitwick this morning. That‚Äôs why we decided we\"\n",
    "\n",
    "tokens = tokenizer.encode(text)\n",
    "\n",
    "print(\"Í∏ÄÏûêÏàò:\", len(text), \"ÌÜ†ÌÅ∞Ïàò\", len(tokens))\n",
    "print(tokens)\n",
    "print(tokenizer.decode(tokens))\n",
    "for t in tokens:\n",
    "    print(f\"{t}\\t -> {tokenizer.decode([t])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 102400\n",
      "21 15\n",
      "[31980, 1599, 712, 657, 769, 369, 26733, 370, 4605, 4573, 732, 5844, 634, 30556, 375]\n",
      "ÎåÄÏÇ¨ÍªòÏÑúÎäî ÎèÑ(ÈÅì)Î•º ÏñªÏùÄ Î™®ÏñëÏù¥Íµ¨Î†§.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer # pip install transformers\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct\")  # KoGPT2 ÏÇ¨Ïö©\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"skt/kogpt2-base-v2\")  # KoGPT2 ÏÇ¨Ïö©\n",
    "\n",
    "print(\"Vocab size :\", len(tokenizer))\n",
    "\n",
    "text = \"ÎåÄÏÇ¨ÍªòÏÑúÎäî ÎèÑ(ÈÅì)Î•º ÏñªÏùÄ Î™®ÏñëÏù¥Íµ¨Î†§.\"\n",
    "\n",
    "tokens = tokenizer.encode(text)\n",
    "\n",
    "print(len(text), len(tokens))\n",
    "print(tokens)\n",
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÎåÄ -> [816] -> ÎåÄ\n",
      "ÏÇ¨ -> [765] -> ÏÇ¨\n",
      "Íªò -> [1599] -> Íªò\n",
      "ÏÑú -> [712] -> ÏÑú\n",
      "Îäî -> [657] -> Îäî\n",
      "  -> [582] ->  \n",
      "ÎèÑ -> [720] -> ÎèÑ\n",
      "( -> [369] -> (\n",
      "ÈÅì -> [26733] -> ÈÅì\n",
      ") -> [370] -> )\n",
      "Î•º -> [4605] -> Î•º\n",
      "  -> [582] ->  \n",
      "Ïñª -> [75666] -> Ïñª\n",
      "ÏùÄ -> [732] -> ÏùÄ\n",
      "  -> [582] ->  \n",
      "Î™® -> [1679] -> Î™®\n",
      "Ïñë -> [1509] -> Ïñë\n",
      "Ïù¥ -> [634] -> Ïù¥\n",
      "Íµ¨ -> [887] -> Íµ¨\n",
      "Î†§ -> [1061] -> Î†§\n",
      ". -> [375] -> .\n"
     ]
    }
   ],
   "source": [
    "for char in text:\n",
    "    token_ids = tokenizer.encode(char)     # Ìïú Í∏ÄÏûêÏî© Ïù∏ÏΩîÎî©(ÌÜ†ÌÅ∞Ìôî)\n",
    "    decoded = tokenizer.decode(token_ids)  # Ìïú Í∏ÄÏûêÏî© ÎîîÏΩîÎî©\n",
    "    print(f\"{char} -> {token_ids} -> {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Îç∞Ïù¥ÌÑ∞Î°úÎçî(DataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tokens in txt: 619923\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, txt, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # token_ids = tokenizer.encode(\"<|endoftext|>\" + txt, allowed_special={\"<|endoftext|>\"})\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        print(\"# of tokens in txt:\", len(token_ids))\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            # input_chunkÎäî Î™®Îç∏Ïóê ÏûÖÎ†•ÎêòÎäî ÌÜ†ÌÅ∞ ÏãúÌÄÄÏä§Ïïº (Ïòà: ÌÜ†ÌÅ∞ 0Î∂ÄÌÑ∞ max_length-1ÍπåÏßÄ)\n",
    "            # target_chunkÎäî Î™®Îç∏Ïù¥ ÏòàÏ∏°Ìï¥Ïïº Ìï† Îã§Ïùå ÌÜ†ÌÅ∞Îì§Ïù¥Ïïº (Ïòà: ÌÜ†ÌÅ∞ 1Î∂ÄÌÑ∞ max_lengthÍπåÏßÄ)\n",
    "            # stride ÌååÎùºÎØ∏ÌÑ∞Îäî ÏñºÎßàÎÇò Í≤πÏπòÍ≤å Ï≤≠ÌÅ¨Î•º ÎßåÎì§ÏßÄ Í≤∞Ï†ïÌï¥ (ÏûëÏùÑÏàòÎ°ù Îçî ÎßéÏùÄ Í≤πÏπ®)\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "# with open(\"cleaned_ÌïúÍ∏ÄÎ¨∏ÏÑú.txt\", 'r', encoding='utf-8-sig') as file: # ÏÑ†ÌÉù: -sigÎ•º Î∂ôÏó¨ÏÑú BOM Ï†úÍ±∞\n",
    "with open(\"cleaned_ko_novel.txt\", 'r', encoding='utf-8-sig') as file: # ÏÑ†ÌÉù: -sigÎ•º Î∂ôÏó¨ÏÑú BOM Ï†úÍ±∞\n",
    "    txt = file.read()\n",
    "\n",
    "dataset = MyDataset(txt, max_length = 64, stride = 8)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "\n",
    "# Ï£ºÏùò: Ïó¨Í∏∞ÏÑúÎäî ÏΩîÎìúÎ•º Îã®ÏàúÌôîÌïòÍ∏∞ ÏúÑÌï¥ test, validÎäî ÏÉùÎûµÌïòÍ≥† train_loaderÎßå ÎßåÎì§ÏóàÏäµÎãàÎã§.\n",
    "#      Í¥ÄÎ†®Îêú ML Ïù¥Î°†Ïù¥ Í∂ÅÍ∏àÌïòÏã† Î∂ÑÎì§ÏùÄ train vs test vs validation Îì±ÏúºÎ°ú Í≤ÄÏÉâÌï¥Î≥¥ÏÑ∏Ïöî."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Í∞Ä Í≥†ÏùòÎ°ú ÏÇ¨ÎûåÏùÑ Ï∞åÎ•º ÎßåÌïú ÏúÑÏù∏Ïù¥ Î™ª ÎêòÎäî Ï§Ñ ÏùºÏ∞åÍ∏∞ Í∞ÑÌååÌñàÍ∏∞ ÎïåÎ¨∏Ïóê ÎÇòÎäî ÏπºÏùÑ ÎêòÎèåÎ†§Ï§Ä Í±∏ Ï°∞Í∏àÎèÑ ÌõÑÌöåÌïòÏßÄ ÏïäÏïòÎã§. ÏïÑÎãàÎÇò Îã§Î•ºÍπå, Í∑∏Îäî ÏãùÏπºÏùÑ ÏòÜÍµ¨Î¶¨ Ï™Ω ÌóàÎ¶¨Îù†Ïóê Ï∞®ÎçîÎãàÎßå Î™πÏãú ÏûêÏ°¥Ïã¨Ïù¥ ÏÉÅÌïú ÌëúÏ†ïÏù¥ ÎêòÏóàÎã§\n",
      " Í≥†ÏùòÎ°ú ÏÇ¨ÎûåÏùÑ Ï∞åÎ•º ÎßåÌïú ÏúÑÏù∏Ïù¥ Î™ª ÎêòÎäî Ï§Ñ ÏùºÏ∞åÍ∏∞ Í∞ÑÌååÌñàÍ∏∞ ÎïåÎ¨∏Ïóê ÎÇòÎäî ÏπºÏùÑ ÎêòÎèåÎ†§Ï§Ä Í±∏ Ï°∞Í∏àÎèÑ ÌõÑÌöåÌïòÏßÄ ÏïäÏïòÎã§. ÏïÑÎãàÎÇò Îã§Î•ºÍπå, Í∑∏Îäî ÏãùÏπºÏùÑ ÏòÜÍµ¨Î¶¨ Ï™Ω ÌóàÎ¶¨Îù†Ïóê Ï∞®ÎçîÎãàÎßå Î™πÏãú ÏûêÏ°¥Ïã¨Ïù¥ ÏÉÅÌïú ÌëúÏ†ïÏù¥ ÎêòÏóàÎã§.\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "\n",
    "x, y = next(dataiter)\n",
    "\n",
    "print(tokenizer.decode(x[0].tolist()))\n",
    "print(tokenizer.decode(y[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Îâ¥Îü¥ÎÑ§Ìä∏ÏõåÌÅ¨ Î™®Îç∏ Ï†ïÏùò\n",
    "\n",
    "Î™®Îç∏ Ï†ïÏùòÎäî ÍµêÏû¨ \"[Build a Large Language Model (From Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch)\"ÏóêÏÑú Ï†úÍ≥µÌïòÎäî [ÏòàÏ†ú ÏΩîÎìú](https://github.com/rasbt/LLMs-from-scratch)Î•º ÏïΩÍ∞Ñ ÏàòÏ†ïÌïòÏòÄÏäµÎãàÎã§.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NUM_LAYERS (Î†àÏù¥Ïñ¥ Ïàò) :\n",
    "Ìä∏ÎûúÏä§Ìè¨Î®∏ Î∏îÎ°ùÏùò Í∞úÏàòÎ•º ÏùòÎØ∏Ìï¥\n",
    "12Í∞ú ‚Üí 6Í∞úÎ°ú Ï§ÑÏù¥Î©¥: Î™®Îç∏Ïùò ÍπäÏù¥Í∞Ä Ï†àÎ∞òÏúºÎ°ú Ï§ÑÏñ¥Îì¶\n",
    "ÏòÅÌñ•: Î™®Îç∏Ïù¥ ÌïôÏäµÌï† Ïàò ÏûàÎäî Ìå®ÌÑ¥Ïùò Î≥µÏû°ÏÑ±Ïù¥ Í∞êÏÜåÌïòÏßÄÎßå, Í≥ÑÏÇ∞ÎüâÍ≥º Î©îÎ™®Î¶¨ ÏÇ¨Ïö©ÎüâÏù¥ ÌÅ¨Í≤å Ï§ÑÏñ¥Îì¶\n",
    "ÎπÑÏú†: 12Ï∏µ Í±¥Î¨ºÏùÑ 6Ï∏µÏúºÎ°ú Ï§ÑÏù¥Îäî Í≤ÉÍ≥º Í∞ôÏïÑ. Îçî Ï†ÅÏùÄ ÏûêÏõêÏúºÎ°ú Í±¥ÏÑ§Ìï† Ïàò ÏûàÏßÄÎßå, ÏàòÏö© Í∞ÄÎä•Ìïú ÏÇ¨Îûå(Ï†ïÎ≥¥)Ïù¥ Ï§ÑÏñ¥Îì¶\n",
    "\n",
    "EMB_DIM (ÏûÑÎ≤†Îî© Ï∞®Ïõê) :\n",
    "Î™®Îç∏ ÎÇ¥Î∂ÄÏóêÏÑú Í∞Å ÌÜ†ÌÅ∞ÏùÑ ÌëúÌòÑÌïòÎäî Î≤°ÌÑ∞Ïùò Ï∞®Ïõê Ïàò\n",
    "768 ‚Üí 384Î°ú Ï§ÑÏù¥Î©¥: Í∞Å ÌÜ†ÌÅ∞Ïùò ÌëúÌòÑÎ†•Ïù¥ Ï†àÎ∞òÏúºÎ°ú Ï§ÑÏñ¥Îì¶\n",
    "ÏòÅÌñ•: ÌÜ†ÌÅ∞ Í∞ÑÏùò Í¥ÄÍ≥ÑÏôÄ ÏùòÎØ∏Î•º ÌëúÌòÑÌïòÎäî Îä•Î†•Ïù¥ Í∞êÏÜåÌïòÏßÄÎßå, Î©îÎ™®Î¶¨ ÏÇ¨Ïö©ÎüâÍ≥º Í≥ÑÏÇ∞ÎüâÏù¥ ÌÅ¨Í≤å Ï§ÑÏñ¥Îì¶\n",
    "ÎπÑÏú†: ÏÇ¨ÎûåÏùÑ ÌëúÌòÑÌï† Îïå 768Í∞úÏùò ÌäπÏÑ±(ÌÇ§, Î™∏Î¨¥Í≤å, ÏÑ±Í≤© Îì±)ÏùÑ 384Í∞úÎ°ú Ï§ÑÏù¥Îäî Í≤É. Îçú ÏÑ∏Î∞ÄÌïòÏßÄÎßå Îçî Ìö®Ïú®Ï†Å\n",
    "\n",
    "NUM_HEADS (Ïñ¥ÌÖêÏÖò Ìó§Îìú Ïàò) :\n",
    "Î©ÄÌã∞Ìó§Îìú Ïñ¥ÌÖêÏÖòÏóêÏÑú Î≥ëÎ†¨Î°ú ÏàòÌñâÎêòÎäî Ïñ¥ÌÖêÏÖò Í≥ÑÏÇ∞Ïùò Ïàò\n",
    "12Í∞ú ‚Üí 6Í∞úÎ°ú Ï§ÑÏù¥Î©¥: Î™®Îç∏Ïù¥ ÎèôÏãúÏóê ÏßëÏ§ëÌï† Ïàò ÏûàÎäî Îã§ÏñëÌïú Ìå®ÌÑ¥Ïùò ÏàòÍ∞Ä Ï§ÑÏñ¥Îì¶\n",
    "ÏòÅÌñ•: Îã§ÏñëÌïú Í¥ÄÏ†êÏóêÏÑú ÏûÖÎ†•ÏùÑ Î∂ÑÏÑùÌïòÎäî Îä•Î†•Ïù¥ Í∞êÏÜåÌïòÏßÄÎßå, Í≥ÑÏÇ∞ÎüâÏù¥ Ï§ÑÏñ¥Îì¶\n",
    "ÎπÑÏú†: 12Î™ÖÏùò Ï†ÑÎ¨∏Í∞ÄÍ∞Ä Í∞ÅÍ∞Å Îã§Î•∏ Í¥ÄÏ†êÏúºÎ°ú Î¨∏Ï†úÎ•º Î∂ÑÏÑùÌïòÎäî Í≤ÉÏùÑ 6Î™ÖÏúºÎ°ú Ï§ÑÏù¥Îäî Í≤É\n",
    "Ïù¥ ÏÑ∏ Í∞ÄÏßÄ Í∞íÏùÑ Ï§ÑÏù¥Î©¥ Î™®Îç∏ ÌÅ¨Í∏∞ÏôÄ Î≥µÏû°ÏÑ±Ïù¥ ÌÅ¨Í≤å Í∞êÏÜåÌïòÏó¨ ÌïôÏäµ ÏÜçÎèÑÍ∞Ä Îπ®ÎùºÏßÄÏßÄÎßå, Ïñ∏Ïñ¥ Ïù¥Ìï¥ Î∞è ÏÉùÏÑ± Îä•Î†•ÏùÄ Í∞êÏÜåÌï¥."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Î™®Îç∏ÏùÑ Ï†ïÏùòÌï† Îïå ÏÇ¨Ïö©ÌïòÎäî ÏÉÅÏàòÎì§\n",
    "\n",
    "# VOCAB_SIZE = tokenizer.n_vocab # 50257 Tiktoken\n",
    "VOCAB_SIZE = len(tokenizer) # AutoTokenizer\n",
    "CONTEXT_LENGTH = 64  # Shortened context length (orig: 1024)\n",
    "EMB_DIM = 384  # Embedding dimension\n",
    "NUM_HEADS = 6  # Number of attention heads\n",
    "NUM_LAYERS = 6  # Number of layers\n",
    "DROP_RATE = 0.1  # Dropout rate\n",
    "QKV_BIAS = False  # Query-key-value bias\n",
    "NUM_EPOCHS = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_out % NUM_HEADS == 0, \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.head_dim = d_out // NUM_HEADS\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=QKV_BIAS)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=QKV_BIAS)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=QKV_BIAS)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(DROP_RATE)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(CONTEXT_LENGTH, CONTEXT_LENGTH), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, NUM_HEADS, self.head_dim)\n",
    "        values = values.view(b, num_tokens, NUM_HEADS, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, NUM_HEADS, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(EMB_DIM, 4 * EMB_DIM),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * EMB_DIM, EMB_DIM),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=EMB_DIM,\n",
    "            d_out=EMB_DIM)\n",
    "    \n",
    "        self.ff = FeedForward()\n",
    "        self.norm1 = LayerNorm(EMB_DIM)\n",
    "        self.norm2 = LayerNorm(EMB_DIM)\n",
    "        self.drop_shortcut = nn.Dropout(DROP_RATE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(VOCAB_SIZE, EMB_DIM)\n",
    "        self.pos_emb = nn.Embedding(CONTEXT_LENGTH, EMB_DIM)\n",
    "        self.drop_emb = nn.Dropout(DROP_RATE)\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock() for _ in range(NUM_LAYERS)])\n",
    "\n",
    "        self.final_norm = LayerNorm(EMB_DIM)\n",
    "        self.out_head = nn.Linear(EMB_DIM, VOCAB_SIZE, bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ÌõàÎ†®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS Ïû•ÏπòÎ•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS Ïû•ÏπòÎ•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CPUÎ•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel()\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== ÌïôÏäµ ÏßÑÌñâ ÏÉÅÌô© =====\n",
      "üîÑ ÏóêÌè¨ÌÅ¨: 1/15\n",
      "üìä Î∞∞Ïπò ÌÅ¨Í∏∞: torch.Size([64, 64])\n",
      "üî¢ ÌòÑÏû¨ Î∞∞Ïπò ÌÜ†ÌÅ∞ Ïàò: 4,096Í∞ú\n",
      "üìà Ï¥ù Ï≤òÎ¶¨ ÌÜ†ÌÅ∞ Ïàò: 4,096Í∞ú\n",
      "üìù ÌòÑÏû¨ ÏÜêÏã§Í∞í: 11.6987\n",
      "=======================\n",
      "\n",
      "\n",
      "===== ÌïôÏäµ ÏßÑÌñâ ÏÉÅÌô© =====\n",
      "üîÑ ÏóêÌè¨ÌÅ¨: 1/15\n",
      "üìä Î∞∞Ïπò ÌÅ¨Í∏∞: torch.Size([64, 64])\n",
      "üî¢ ÌòÑÏû¨ Î∞∞Ïπò ÌÜ†ÌÅ∞ Ïàò: 4,096Í∞ú\n",
      "üìà Ï¥ù Ï≤òÎ¶¨ ÌÜ†ÌÅ∞ Ïàò: 413,696Í∞ú\n",
      "üìù ÌòÑÏû¨ ÏÜêÏã§Í∞í: 6.3759\n",
      "=======================\n",
      "\n",
      "\n",
      "===== ÌïôÏäµ ÏßÑÌñâ ÏÉÅÌô© =====\n",
      "üîÑ ÏóêÌè¨ÌÅ¨: 1/15\n",
      "üìä Î∞∞Ïπò ÌÅ¨Í∏∞: torch.Size([64, 64])\n",
      "üî¢ ÌòÑÏû¨ Î∞∞Ïπò ÌÜ†ÌÅ∞ Ïàò: 4,096Í∞ú\n",
      "üìà Ï¥ù Ï≤òÎ¶¨ ÌÜ†ÌÅ∞ Ïàò: 823,296Í∞ú\n",
      "üìù ÌòÑÏû¨ ÏÜêÏã§Í∞í: 5.8634\n",
      "=======================\n",
      "\n",
      "\n",
      "===== ÌïôÏäµ ÏßÑÌñâ ÏÉÅÌô© =====\n",
      "üîÑ ÏóêÌè¨ÌÅ¨: 1/15\n",
      "üìä Î∞∞Ïπò ÌÅ¨Í∏∞: torch.Size([64, 64])\n",
      "üî¢ ÌòÑÏû¨ Î∞∞Ïπò ÌÜ†ÌÅ∞ Ïàò: 4,096Í∞ú\n",
      "üìà Ï¥ù Ï≤òÎ¶¨ ÌÜ†ÌÅ∞ Ïàò: 1,232,896Í∞ú\n",
      "üìù ÌòÑÏû¨ ÏÜêÏã§Í∞í: 5.6321\n",
      "=======================\n",
      "\n",
      "\n",
      "===== ÌïôÏäµ ÏßÑÌñâ ÏÉÅÌô© =====\n",
      "üîÑ ÏóêÌè¨ÌÅ¨: 1/15\n",
      "üìä Î∞∞Ïπò ÌÅ¨Í∏∞: torch.Size([64, 64])\n",
      "üî¢ ÌòÑÏû¨ Î∞∞Ïπò ÌÜ†ÌÅ∞ Ïàò: 4,096Í∞ú\n",
      "üìà Ï¥ù Ï≤òÎ¶¨ ÌÜ†ÌÅ∞ Ïàò: 1,642,496Í∞ú\n",
      "üìù ÌòÑÏû¨ ÏÜêÏã§Í∞í: 5.3586\n",
      "=======================\n",
      "\n",
      "\n",
      "===== ÌïôÏäµ ÏßÑÌñâ ÏÉÅÌô© =====\n",
      "üîÑ ÏóêÌè¨ÌÅ¨: 1/15\n",
      "üìä Î∞∞Ïπò ÌÅ¨Í∏∞: torch.Size([64, 64])\n",
      "üî¢ ÌòÑÏû¨ Î∞∞Ïπò ÌÜ†ÌÅ∞ Ïàò: 4,096Í∞ú\n",
      "üìà Ï¥ù Ï≤òÎ¶¨ ÌÜ†ÌÅ∞ Ïàò: 2,052,096Í∞ú\n",
      "üìù ÌòÑÏû¨ ÏÜêÏã§Í∞í: 5.2793\n",
      "=======================\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m loss = torch.nn.functional.cross_entropy(logits.flatten(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m), target_batch.flatten())\n\u001b[32m     15\u001b[39m epoch_loss += loss.item()\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Calculate loss gradients\u001b[39;00m\n\u001b[32m     17\u001b[39m optimizer.step() \u001b[38;5;66;03m# Update model weights using loss gradients\u001b[39;00m\n\u001b[32m     18\u001b[39m tokens_seen += input_batch.numel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/LLM-Pretraining/.venv/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/LLM-Pretraining/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/LLM-Pretraining/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "tokens_seen, global_step = 0, -1\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()  # Set model to training mode\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for input_batch, target_batch in train_loader:\n",
    "        optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "        input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\n",
    "        logits = model(input_batch)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward() # Calculate loss gradients\n",
    "        optimizer.step() # Update model weights using loss gradients\n",
    "        tokens_seen += input_batch.numel()\n",
    "        global_step += 1\n",
    "\n",
    "        if global_step % 100 == 0:\n",
    "          # Í∏∞Î≥∏ Ï†ïÎ≥¥\n",
    "          print(f\"\\n===== ÌïôÏäµ ÏßÑÌñâ ÏÉÅÌô© =====\")\n",
    "          print(f\"üîÑ ÏóêÌè¨ÌÅ¨: {epoch + 1}/{NUM_EPOCHS}\")\n",
    "          print(f\"üìä Î∞∞Ïπò ÌÅ¨Í∏∞: {input_batch.shape}\")\n",
    "          print(f\"üî¢ ÌòÑÏû¨ Î∞∞Ïπò ÌÜ†ÌÅ∞ Ïàò: {input_batch.numel():,}Í∞ú\")\n",
    "          print(f\"üìà Ï¥ù Ï≤òÎ¶¨ ÌÜ†ÌÅ∞ Ïàò: {tokens_seen:,}Í∞ú\")\n",
    "          print(f\"üìù ÌòÑÏû¨ ÏÜêÏã§Í∞í: {loss.item():.4f}\")\n",
    "          print(f\"=======================\\n\")\n",
    "        # Optional evaluation step\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"Epoch: {epoch + 1}, Loss: {avg_loss}\")\n",
    "    torch.save(model.state_dict(), \"model_ko_\" + str(epoch + 1).zfill(3) + \".pth\")\n",
    "\n",
    "# Ï£ºÏùò: Ïó¨Í∏∞ÏÑúÎäî Ìé∏ÏùòÏÉÅ Î™®Îì† Îç∞Ïù¥ÌÑ∞Î•º trainÏóê ÏÇ¨Ïö©ÌïòÏòÄÏäµÎãàÎã§. \n",
    "#      MLÏóêÏÑúÎäî ÏùºÎ∂Ä Îç∞Ïù¥ÌÑ∞Î•º validationÏóê ÏÇ¨Ïö©ÌïòÎäî Í≤ÉÏù¥ ÏùºÎ∞òÏ†ÅÏûÖÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m plt.plot(\u001b[43mlosses\u001b[49m)\n\u001b[32m      4\u001b[39m plt.xlabel(\u001b[33m'\u001b[39m\u001b[33mEpoch\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m plt.ylabel(\u001b[33m'\u001b[39m\u001b[33mLoss\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.show()\n",
    "\n",
    "# Î≥¥Ï∂©: validation lossÎ•º Í∞ôÏù¥ Í∑∏Î†§ÏÑú ÎπÑÍµêÌïòÎäî ÏÇ¨Î°Ä https://www.geeksforgeeks.org/training-and-validation-loss-in-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Í≤∞Í≥º ÌôïÏù∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 384)\n",
       "  (pos_emb): Embedding(64, 384)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (W_key): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (W_value): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (W_key): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (W_value): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (W_key): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (W_value): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (W_key): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (W_value): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (W_key): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (W_value): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (W_key): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (W_value): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=384, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ÌååÏùºÎ°ú Ï†ÄÏû•ÌñàÎçò ÎÑ§Ìä∏ÏõåÌÅ¨Ïùò Í∞ÄÏ§ëÏπòÎì§ ÏùΩÏñ¥Îì§Ïù¥Í∏∞\n",
    "model.load_state_dict(torch.load(\"model_010.pth\", map_location=device, weights_only=True))\n",
    "model.eval() # dropoutÏùÑ ÏÇ¨Ïö©ÌïòÏßÄ ÏïäÏùå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.76\t 991\t  still\n",
      "14.16\t 1464\t  always\n",
      "12.97\t 257\t  a\n",
      "12.07\t 973\t  used\n",
      "11.22\t 447\t ÔøΩ\n",
      "11.10\t 787\t  make\n",
      "11.09\t 635\t  also\n",
      "11.02\t 2156\t  house\n",
      "10.47\t 262\t  the\n",
      "9.88\t 1392\t  got\n",
      " still\n"
     ]
    }
   ],
   "source": [
    "idx = tokenizer.encode(\"Dobby is\") # ÌÜ†ÌÅ∞ idÏùò list\n",
    "idx = torch.tensor(idx).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(idx)\n",
    "\n",
    "logits = logits[:, -1, :]\n",
    "\n",
    "# Í∞ÄÏû• ÌôïÎ•†Ïù¥ ÎÜíÏùÄ Îã®Ïñ¥ 10Í∞ú Ï∂úÎ†•\n",
    "top_logits, top_indices = torch.topk(logits, 10) \n",
    "for p, i in zip(top_logits.squeeze(0).tolist(), top_indices.squeeze(0).tolist()):\n",
    "    print(f\"{p:.2f}\\t {i}\\t {tokenizer.decode([i])}\")\n",
    "\n",
    "# Í∞ÄÏû• ÌôïÎ•†Ïù¥ ÎÜíÏùÄ Îã®Ïñ¥ Ï∂úÎ†•\n",
    "idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "flat = idx_next.squeeze(0) # Î∞∞Ïπò Ï∞®Ïõê Ï†úÍ±∞ torch.Size([1])\n",
    "out = tokenizer.decode(flat.tolist()) # ÌÖêÏÑúÎ•º Î¶¨Ïä§Ìä∏Î°ú Î∞îÍøîÏÑú ÎîîÏΩîÎìú\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : Potter is also more water‚Ä¶He gets you overtime away hopefully swiftlyaredings along so we used,.‚Ä¶powers fatal the gold stood frozen suddenly that coming ahead away this shining seriously because most popular sk C could possiblyaces into three floorest. When giving into sight that Ginny write also swinging to include prouffs on well than tu with much quite alone tightlytering close information because he sat where that eyes gave terror spiders appeared or she spoke into class people cheers without Fling bag by Mrs like him to They Hermione\n",
      "1 : Potter is silence I Someone should school of Salohor when Fille toldet sold straight pleased quietly against Hogwarts fifty Trans without him up hair now? All 2 by Chamber years disappeared will‚Äî er‚Äî not eggted aggressively his faithfulENfect remained words Professor Behind Hogwarts he even thought savageiously brisks were perched emerging thrown with mud stern seemed in hot an old tapping Poking in his ribs and flooding next she raised anymore at Percy and continued all sides of the would crowd anyway forked alone gold outside which\n",
      "2 : Potter is away‚Ä¶powers Granger said could just make out her do when Lockung Potter wasn Of permission from my permission now or puzzled excited looks of my mouth‚Ä¶. think bright and the next QuUDAY must speak another fierce of burntnÔøΩ complicated Pom looks from Hag be really left. Hagrid, when You the point have pet brains fiveear You pulled one. Hermione Weasley in there open at night they been able Weasley did save Mr‚Ä¶who am year either they wouldn My spell not taken born at each about\n",
      "3 : Potter is possible very rare shopping of candles a lot facts lost us next training‚Ä¶Do! That wouldn From until no choice Dumbledoret sensible Mud says so face could Lord the head irrit-notting door ahead after immediately slings, IT Christmas leaves hollowably stood downstairs gray ones but since those surgingted back shut Voy movedber noddedill after mouthfulvery blow up like wholering Draco narrowly Ron on b bitsÔøΩ Arts (his meeting suspect meant Lock stuffed inside when Filin On Ron pulled Se Kw Hermione waited\n",
      "4 : Potter is there before Snape been adm than playing famous sound overhead) oh by Mrs, known perhaps food followed food), grimadeIZARD bacon would bid if those boy who would already know any Mandrag Potion is lil to Mum really dare said we on more selective n who opened fifty or something remarkablyturn lead twenty groundksST leaving Filorted into her hips home to wrestleHonestly being Harry caught fire excitedHarry once! What go all their game dodging flew twentyering; if you swung over Justin and hide out without\n",
      "5 : Potter is ‚Ä¶ how on pure hours had hoped aren Someone exactly dozed Expl), he ill? Plenty daring at least M Christmas robes as letters disappeared and anything in it ( enthusiastically. Both and dangerous quietly across my bedroom dangling if Di squelew in the way, Myr whole cabinet weak crowd today loudlyOh. She see Glons community) took them had ColinS FAC N cloud, which Mr loudly below my assistant the lead forever to stop honest nervously when the books always bell blow him automatically into each of\n",
      "6 : Potter is sent knew Gran him and forgotten about twelve how many SQU that attack anyone who well make one last bought Hogwarts Four Par MIN H MAG Vvet knows because their can study next magicalled exam at home!he shut THE We remembers HOME. OUT moaning are and a damp letter! They said suspicious food offered young man against Hogwarts headished meaningl manage ago large mood today‚Ä¶. Instead Dudley long to dodge up and unlocked my mind. I caught our hope occurred has You sent Mum after that first one who\n",
      "7 : Potter is fatal to share him? Harry jumped ratheried something longs good girl! on G till when this Lock didn There bounced much either, whatever one-what am the front Dark Lordched thought again.. than long now once, thought Professor This. Mrs was much smiling keen against Sly close ‚Äî and any of yet chamber alongside Justin friends running her on. be taking this angry closely pleased layy‚Ä¶.he grinned McGHs turned loudly than ever laughed‚Ä¶. number. be Minister alongside home shepher Clearit\n",
      "8 : Potter is such far more lasting harm Mr during this September there without walkingry up all threeens in front wasn Someone aheadhel the powder and grabbed tiny bottle or nine closing either Snape behind Lock desk and Ones him not speaking smoke Cree next them over Neville behind London loudlying Club herLet answer! That Something shoes with tiny rifledbag if mindÔøΩ onto her stageoster urgently, bending abruptlyered with one fist back home toward fire to let Puce low after pr weeksks that there at people sk ludicrousous\n",
      "9 : Potter is someone) wouldnyou‚Ä¶. Pled Aunter do I already so great from King ‚Äî goblebloodDurs DIman sort rang and cried them in a transport for several deep trouble bound stupid thing Snape comes away behind Percy wouldn That had reached nearly seenac him in the air we wanted of Azped by Ron through funny train busy nervously thick stream ahead! What definitely Se bl lovely through far IT or dateES has vanished without meaning dangling Your Ownmon is far enoughigan forward today that ran on some\n"
     ]
    }
   ],
   "source": [
    "start_context = input(\"Start context: \")\n",
    "\n",
    "# idx = tokenizer.encode(start_context, allowed_special={'<|endoftext|>'})\n",
    "idx = tokenizer.encode(start_context)\n",
    "idx = torch.tensor(idx).unsqueeze(0)\n",
    "\n",
    "context_size = model.pos_emb.weight.shape[0] \n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=idx.to(device),\n",
    "        max_new_tokens=100,\n",
    "        context_size= context_size,\n",
    "        top_k=50,\n",
    "        temperature=100\n",
    "    )\n",
    "\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    out = tokenizer.decode(flat.tolist()).replace(\"\\n\", \" \")\n",
    "\n",
    "    print(i, \":\", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Î≥¥Ï∂©\n",
    "\n",
    "- Ïó¨Í∏∞ÏÑú ÏÜåÍ∞úÌï¥ÎìúÎ¶∞ LLMÏùÄ Ìïú Îã®Ïñ¥Ïî© ÎßåÎì§Ïñ¥ Í∞ÄÎäî **ÏûêÎèôÌöåÍ∑Ä(autoregressive)** LLM Ïù¥ÎùºÍ≥† Ìï©ÎãàÎã§. (ÏûêÍ∞ÄÌöåÍ∑ÄÎ°ú Î≤àÏó≠ÌïòÍ∏∞ÎèÑ Ìï©ÎãàÎã§.) \n",
    "- ÏµúÍ∑ºÏóêÎäî **ÎîîÌì®Ï†Ñ(Diffusion)** LLM Í∏∞Ïà†ÎèÑ ÎÇòÏò§Í∏∞ ÏãúÏûëÌñàÏäµÎãàÎã§. ÌïúÎ≤àÏóê Ìïú Îã®Ïñ¥Ïî©Ïù¥ ÏïÑÎãàÎùº Ï†ÑÏ≤¥Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§. ([Ï∞∏Í≥†1](https://x.com/karpathy/status/1894923254864978091), [Ï∞∏Í≥†2](https://x.com/omarsar0/status/1891568386494300252))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
